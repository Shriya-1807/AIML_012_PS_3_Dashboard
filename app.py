# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P8d84K3HrE8bySVIucRiJ6uNHPUVcHIa
"""

import streamlit as st
import numpy as np
import tempfile
import os
import cv2
from PIL import Image
from collections import Counter
import time
import torch
import uuid
import shutil
import gc

from ultralytics import YOLOWorld
from sahi.predict import get_sliced_prediction
from sahi.models.ultralytics import UltralyticsDetectionModel
from deep_sort_realtime.deepsort_tracker import DeepSort

st.set_page_config(page_title='Drone Detector', page_icon='üöÄ', layout='wide')

# Styling
st.markdown("""
    <style>
        .main {max-height: 100vh; overflow-y: scroll;}
        .block-container {padding-top: 0rem !important; margin-top: 0rem !important;}
        .title {font-size: 36px; font-family: Georgia, serif; text-align: center; }
        section[data-testid="stSidebar"] {
            background:linear-gradient(to bottom , #0F202B, #202D4A, #172626);
            padding: 20px;
            border-right: 2px solid #FFFFFF;
            font-family: Verdana, sans-serif;
            font-size: 16px;
            width: 300px !important;
        }
        div[data-testid="stSidebarContent"] {width: 100% !important;}
        main{background: linear-gradient(to bottom right, #0F202B, #020229 );padding: 10px;}
        .stTabs [data-baseweb="tab-list"] {
            gap: 24px;
        }
        .stTabs [data-baseweb="tab"] {
            height: 50px;
            white-space: pre-wrap;
            background-color: rgba(255, 255, 255, 0.1);
            border-radius: 4px 4px 0px 0px;
            gap: 1px;
            padding-top: 10px;
            padding-bottom: 10px;
        }
        .stTabs [aria-selected="true"] {
            background-color: #8cc8e6;
            color: #000000;
        }
    </style>
""", unsafe_allow_html=True)

st.markdown('<h1 class="title">Drone Footage Object Detection and Tracking</h1>', unsafe_allow_html=True)
tab1, tab2 = st.tabs(["üì∏ Image Processing", "üé• Video Processing"])

# Sidebar
st.sidebar.markdown(" Model Configuration")

selected_model = st.sidebar.radio(
    label="Select",
    options=["Default Detection","Text-prompt Detection"],
    index=0,
    help="Select desired option"
)

confidence_value = st.sidebar.slider("Select model confidence value", min_value=0.1, max_value=1.0, value=0.25, step=0.05)

model_path = "last.pt"
text_prompt_model_path = "yolov8l-worldv2.pt"

category_names = []
if selected_model == "Text-prompt Detection":
    st.sidebar.markdown("---")
    st.sidebar.markdown("**Text Prompt Settings**")
    user_prompts = st.sidebar.text_input(
        "Enter class names (separated by comma):",
        help="Enter the objects you want to detect, separated by commas"
    )
    category_names = [x.strip() for x in user_prompts.split(",") if x.strip() != ""]
    
    if category_names:
        st.sidebar.write("**Classes to detect:**")
        for i, name in enumerate(category_names, 1):
            st.sidebar.write(f"{i}. {name}")

# Image

def run_sahi_yolo_inference(image_pil, model_path, conf):
    image_np = np.array(image_pil.convert("RGB"))
    detection_model = UltralyticsDetectionModel(
        model_path=model_path,
        confidence_threshold=conf,
        device="cuda:0" if torch.cuda.is_available() else "cpu"
    )
    result = get_sliced_prediction(
        image_np,
        detection_model,
        slice_height=512,
        slice_width=512,
        overlap_height_ratio=0.2,
        overlap_width_ratio=0.2
    )
    return result
def run_text_prompt_sahi_inference(image_pil, model_path, conf, category_names):
    """Text prompt SAHI inference with your colab settings"""
    image_np = np.array(image_pil.convert("RGB"))
    
    
    model = YOLOWorld(text_prompt_model_path)
    model.set_classes(category_names)
    
    detection_model = UltralyticsDetectionModel(
        model=model,
        confidence_threshold=conf,
        device="cuda:0" if torch.cuda.is_available() else "cpu"
    )
    
    
    result = get_sliced_prediction(
        image_np,
        detection_model,
        slice_height=256,  # Using your colab settings
        slice_width=256,
        overlap_height_ratio=0.3,
        overlap_width_ratio=0.3
    )
    return result

with tab1:
    st.markdown(
    '<p style="font-size:22px; font-family:\'Segoe UI\', sans-serif; font-weight:bold; color:#8cc8e6; margin-top:2px;">üì∏ Upload a drone image</p>',
    unsafe_allow_html=True
    )
    uploaded_image = st.file_uploader("Upload Image", type=['jpg', 'jpeg', 'png', 'webp'], key="img_upload") 
    

    if uploaded_image is not None:
        image = Image.open(uploaded_image)
        st.markdown("### üñºÔ∏è Uploaded Image Preview")
        st.image(image, caption="Uploaded Image", use_container_width=True)

    
        if selected_model == "Text-prompt Detection" and category_names:
            st.info(f"**Detecting:** {', '.join(category_names)}")
        elif selected_model == "Text-prompt Detection" and not category_names:
            st.warning(" Please enter class names in the sidebar for text prompt detection!")
            st.stop()

        with st.spinner("Running SAHI tiled inference..."):
            result = None
            result_img_path = None
        
            if selected_model == "Default Detection":
                result = run_sahi_yolo_inference(image, model_path, confidence_value)
            else:  
                result = run_text_prompt_sahi_inference(image, text_prompt_model_path, confidence_value, category_names)

            if result is None:
                st.error(" Inference Failed!" )
            else:
                unique_img_name = f"result_{uuid.uuid4().hex}"
                output_dir = os.path.abspath("outputs")
                os.makedirs(output_dir, exist_ok=True)
                result_img_path = os.path.join(output_dir, f"{unique_img_name}.png")

                try:
                    result.export_visuals(
                        export_dir = output_dir,   
                        file_name=unique_img_name,
                        text_size=0.5,
                        rect_th=1,
                        hide_labels=False,
                        hide_conf=True,
                    )
                    time.sleep(1)
                    if os.path.exists(result_img_path):
                        st.success("Inference done and Image exported successfully!")
                    else:
                        st.error("Failed to export")
                        result_img_path = None
                        
                except Exception as e:
                    st.error(f"Failed to export result visualization: {e}")
                    result_img_path = None
        
          
    
    if result_img_path and os.path.exists(result_img_path):
        st.markdown("### üéØ Detected Output")
        with open(result_img_path, 'rb') as f:
            img_bytes = f.read()
        st.image(img_bytes, caption="Detected with SAHI", use_container_width=True)
        
        st.download_button(
            label="üì• Download Annotated Image",
            data=img_bytes,
            file_name=os.path.basename(result_img_path),
            mime="image/jpeg"
        )
        
        st.markdown("### üìä Object Counts")
        class_names = [pred.category.name for pred in result.object_prediction_list]
        class_counts = Counter(class_names)
        for cls, count in class_counts.items():
            st.markdown(f"- **{cls}**: {count}")

    else:
        st.error("‚ùå Exported image not found.")

#Video
skip_frames = 2 

def process_video_with_yolo_deepsort(video_path, output_path, conf, selected_model, category_names=None):
    
    try:
        if selected_model == "Default Detection":
            model = YOLOWorld(model_path)
        else:
            if not category_names:
                st.error("Please enter class names for text-prompt detection")
                return None, None
            model = YOLOWorld(text_prompt_model_path)
            model.set_classes(category_names)
        
        model.model.to("cpu")
        tracker = DeepSort(max_age=10)
        
        # Open video
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            st.error("Could not open the uploaded video. Please try with a different .mp4 file.")
            return None, None
        
        # Get video properties
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if fps is None or fps <= 0 or np.isnan(fps):
            fps = 24.0
        
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        frame_count = 0
        prev_tracks = []
        detection_counts = Counter()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
                  
            if frame_count % skip_frames == 0:
                try:
                    results = model.predict(frame, conf=conf, iou=0.6, augment=False, verbose=False)
                    if results and len(results)>0:
                        results = results[0]
                    
                        detections = []
                        if results.boxes is not None and len(results.boxes)>0:
                            for box in results.boxes:
                                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
                                conf_score = float(box.conf[0].cpu().numpy())
                                cls = int(box.cls[0].cpu().numpy())
                            
                            # count detections
                                if selected_model == "Default Detection":
                                    class_name = model.names[cls] if cls < len(model.names) else f"class_{cls}"
                                else:
                                    class_name = category_names[cls] if cls < len(category_names) else f"class_{cls}"
                            
                                detection_counts[class_name] += 1
                                detections.append(([x1, y1, x2 - x1, y2 - y1], conf_score, cls))
                    
                        # tracker
                        tracks = tracker.update_tracks(detections, frame=frame)
                        prev_tracks = tracks
                    else:
                        tracks = prev_tracks
                    
                except Exception as e:
                    tracks = prev_tracks   
            else:
                tracks = prev_tracks
            
            # tracking results
            for track in tracks:
                if not track.is_confirmed():
                    continue
                    
                l, t, r, b = map(int, track.to_ltrb())
                track_id = track.track_id
                
                # Draw bounding box and ID
                cv2.rectangle(frame, (l, t), (r, b), (0, 255, 0), 2)
                cv2.putText(frame, f"ID: {track_id}", (l, t - 10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            out.write(frame)
        
        # Cleanup
        cap.release()
        out.release()
        
        return output_path, detection_counts
        
    except Exception as e:
        st.error(f"Error processing video: {str(e)}")
        return None, None
    finally:
        if 'cap' in locals():
            cap.release()
        if 'out' in locals():
            out.release()
        gc.collect()
        
with tab2:
    st.markdown(
        '<p style="font-size:22px; font-family:\'Segoe UI\', sans-serif; font-weight:bold; color:#8cc8e6; margin-top:2px;">üé• Upload a drone video</p>',
        unsafe_allow_html=True
    )

    st.markdown("Select a Video file to upload: ")
    uploaded_video = st.file_uploader("Upload Video", type=['mp4', 'avi', 'mov', 'mkv'], key="vid_upload")
    
    if uploaded_video is not None:
        # Check file size
        file_size = len(uploaded_video.getvalue()) / (1024 * 1024)  # Size in MB
        
        if file_size > max_file_size:
            st.error(f"üö´ File too large! Please upload a video smaller than {max_file_size}MB. Current size: {file_size:.1f}MB")
        else:
            st.markdown("### üñºÔ∏è Uploaded Video Preview")
            st.video(uploaded_video)

            if selected_model == "Text-prompt Detection" and category_names:
                st.info(f"**Detecting:** {', '.join(category_names)}")
            elif selected_model == "Text-prompt Detection" and not category_names:
                st.warning("‚ö†Ô∏è Please enter class names in the sidebar for text prompt detection!")
                

            # Use tempfile for video processing
            with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp_vid:
                video_bytes = uploaded_video.read()
                tmp_vid.write(video_bytes)
                temp_video_path = tmp_vid.name

            with st.spinner("Processing video..."):
                with tempfile.NamedTemporaryFile(delete=False, suffix="_out.mp4") as tmp_out:
                    temp_output_path = tmp_out.name
                
                try:
                    result_path, detection_counts = process_video_with_yolo_deepsort(
                        temp_video_path,
                        output_path=temp_output_path,
                        conf=confidence_value,
                        selected_model=selected_model,
                        category_names=category_names
                    )
                    
                    time.sleep(1)
                    
                    if result_path and os.path.exists(result_path) and os.path.getsize(result_path) > 1000:
                        st.success("‚úÖ Video processed!")
                        
                        st.markdown("### üéØ Processed Video")
                        with open(result_path, 'rb') as vid_file:
                            vid_bytes = vid_file.read()
                        st.video(vid_bytes)
                        
                        st.download_button(
                            label="üì• Download Processed Video",
                            data=vid_bytes,
                            file_name=f"processed_{uploaded_video.name}",
                            mime="video/mp4"
                        )
                        
                        st.markdown("### üìä Object Counts")
                        if detection_counts:
                            for cls, count in detection_counts.items():
                                st.markdown(f"- **{cls}**: {count}")
                        else:
                            st.markdown("No objects detected in the video.")
                    else:
                        st.error("Processed video not found or is empty.")
                        
                except Exception as e:
                    st.error(f"‚ö†Ô∏è Video processing failed: {e}")
                
                # Cleanup temporary files
                try:
                    if os.path.exists(temp_video_path):
                        os.remove(temp_video_path)
                    if os.path.exists(temp_output_path):
                        os.remove(temp_output_path)
                except Exception as e:
                    st.warning(f"Could not cleanup temporary files: {e}")
                    pass

# Footer
st.markdown("---")
st.markdown(
    '<p style="text-align: center; color: #8cc8e6; font-size: 14px;">üöÄ Drone Detection Dashboard | Built with Streamlit & YOLO</p>',
    unsafe_allow_html=True
)


    
