# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P8d84K3HrE8bySVIucRiJ6uNHPUVcHIa
"""

# -*- coding: utf-8 -*-
"""
Drone Footage Object Detection and Tracking App
Fixed version with error corrections
"""

import streamlit as st
import numpy as np
import tempfile
import os
import cv2
from PIL import Image
from collections import Counter
import time
import torch
import uuid
import shutil
import gc

# Import with error handling
try:
    from ultralytics import YOLOWorld
    from sahi.predict import get_sliced_prediction
    from sahi.models.ultralytics import UltralyticsDetectionModel
    from deep_sort_realtime.deepsort_tracker import DeepSort
except ImportError as e:
    st.error(f"Missing required package: {e}")
    st.stop()

# Streamlit Page Config
st.set_page_config(page_title='Drone Detector', page_icon='üöÄ', layout='wide')

# Styling
st.markdown("""
    <style>
        .main {max-height: 100vh; overflow-y: scroll;}
        .block-container {padding-top: 0rem !important; margin-top: 0rem !important;}
        .title {font-size: 36px; font-family: Georgia, serif; text-align: center;}
        section[data-testid="stSidebar"] {
            background: linear-gradient(to bottom, #0F202B, #202D4A, #172626);
            padding: 20px;
            border-right: 2px solid #FFFFFF;
            font-family: Verdana, sans-serif;
            font-size: 16px;
            width: 300px !important;
        }
        div[data-testid="stSidebarContent"] {width: 100% !important;}
        main {background: linear-gradient(to bottom right, #0F202B, #020229); padding: 10px;}
        .stTabs [data-baseweb="tab-list"] {
            gap: 24px;
        }
        .stTabs [data-baseweb="tab"] {
            height: 50px;
            white-space: pre-wrap;
            background-color: rgba(255, 255, 255, 0.1);
            border-radius: 4px 4px 0px 0px;
            gap: 1px;
            padding-top: 10px;
            padding-bottom: 10px;
        }
        .stTabs [aria-selected="true"] {
            background-color: #8cc8e6;
            color: #000000;
        }
    </style>
""", unsafe_allow_html=True)

st.markdown('<h1 class="title">Drone Footage Object Detection and Tracking</h1>', unsafe_allow_html=True)
tab1, tab2 = st.tabs(["üì∏ Image Processing", "üé• Video Processing"])

# Sidebar Configuration
st.sidebar.markdown("## Model Configuration")

selected_model = st.sidebar.radio(
    label="Select Detection Mode",
    options=["Default Detection", "Text-prompt Detection"],
    index=0,
    help="Select desired detection option"
)

confidence_value = st.sidebar.slider(
    "Select model confidence value", 
    min_value=0.1, 
    max_value=1.0, 
    value=0.25, 
    step=0.05
)

# Model paths - Add validation
model_path = "last.pt"
text_prompt_model_path = "yolov8l-worldv2.pt"

# Validate model files exist
if not os.path.exists(model_path):
    st.sidebar.warning(f"‚ö†Ô∏è Default model file '{model_path}' not found!")

if selected_model == "Text-prompt Detection" and not os.path.exists(text_prompt_model_path):
    st.sidebar.warning(f"‚ö†Ô∏è Text prompt model file '{text_prompt_model_path}' not found!")

category_names = []
if selected_model == "Text-prompt Detection":
    st.sidebar.markdown("---")
    st.sidebar.markdown("**Text Prompt Settings**")
    user_prompts = st.sidebar.text_input(
        "Enter class names (separated by comma):",
        help="Enter the objects you want to detect, separated by commas",
        placeholder="person, car, drone"
    )
    category_names = [x.strip() for x in user_prompts.split(",") if x.strip()]
    
    if category_names:
        st.sidebar.write("**Classes to detect:**")
        for i, name in enumerate(category_names, 1):
            st.sidebar.write(f"{i}. {name}")

# Image Processing Functions
def run_sahi_yolo_inference(image_pil, model_path, conf):
    """Run SAHI inference with default YOLO model"""
    try:
        image_np = np.array(image_pil.convert("RGB"))
        
        # Check if model file exists
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found: {model_path}")
        
        detection_model = UltralyticsDetectionModel(
            model_path=model_path,
            confidence_threshold=conf,
            device="cuda:0" if torch.cuda.is_available() else "cpu"
        )
        
        result = get_sliced_prediction(
            image_np,
            detection_model,
            slice_height=512,
            slice_width=512,
            overlap_height_ratio=0.2,
            overlap_width_ratio=0.2
        )
        return result
    except Exception as e:
        st.error(f"Error in SAHI inference: {str(e)}")
        return None

def run_text_prompt_sahi_inference(image_pil, model_path, conf, category_names):
    """Text prompt SAHI inference with YOLO World"""
    try:
        image_np = np.array(image_pil.convert("RGB"))
        
        # Check if model file exists
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found: {model_path}")
        
        # Load YOLO World model
        model = YOLOWorld(model_path)
        model.set_classes(category_names)
        
        detection_model = UltralyticsDetectionModel(
            model=model,
            confidence_threshold=conf,
            device="cuda:0" if torch.cuda.is_available() else "cpu"
        )
        
        result = get_sliced_prediction(
            image_np,
            detection_model,
            slice_height=256,
            slice_width=256,
            overlap_height_ratio=0.3,
            overlap_width_ratio=0.3
        )
        return result
    except Exception as e:
        st.error(f"Error in text prompt inference: {str(e)}")
        return None

# Tab 1: Image Processing
with tab1:
    st.markdown(
        '<p style="font-size:22px; font-family:\'Segoe UI\', sans-serif; font-weight:bold; color:#8cc8e6; margin-top:2px;">üì∏ Upload a drone image</p>',
        unsafe_allow_html=True
    )
    
    uploaded_image = st.file_uploader(
        "Upload Image", 
        type=['jpg', 'jpeg', 'png', 'webp'], 
        key="img_upload"
    ) 
    
    if uploaded_image is not None:
        try:
            image = Image.open(uploaded_image)
            st.markdown("### üñºÔ∏è Uploaded Image Preview")
            st.image(image, caption="Uploaded Image", use_container_width=True)

            # Validation for text prompt detection
            if selected_model == "Text-prompt Detection" and not category_names:
                st.warning("‚ö†Ô∏è Please enter class names in the sidebar for text prompt detection!")
                st.stop()
            elif selected_model == "Text-prompt Detection" and category_names:
                st.info(f"**Detecting:** {', '.join(category_names)}")

            # Run inference
            with st.spinner("Running SAHI tiled inference..."):
                result = None
                
                if selected_model == "Default Detection":
                    result = run_sahi_yolo_inference(image, model_path, confidence_value)
                else:  # Text-prompt Detection
                    result = run_text_prompt_sahi_inference(
                        image, text_prompt_model_path, confidence_value, category_names
                    )
                
                if result is None:
                    st.error("‚ùå Inference failed. Please check your model files and try again.")
                    st.stop()

                # Export results
                unique_img_name = f"result_{uuid.uuid4().hex}"
                output_dir = os.path.abspath("outputs")
                os.makedirs(output_dir, exist_ok=True)
                result_img_path = os.path.join(output_dir, f"{unique_img_name}.png")

                try:
                    result.export_visuals(
                        export_dir=output_dir,   
                        file_name=unique_img_name,
                        text_size=0.5,
                        rect_th=1,
                        hide_labels=False,
                        hide_conf=True,
                    )
                    time.sleep(1)
                    
                    if os.path.exists(result_img_path):
                        st.success("‚úÖ Inference completed and image exported successfully!")
                    else:
                        st.error("‚ùå Failed to export result visualization.")
                        result_img_path = None
                        
                except Exception as e:
                    st.error(f"Failed to export result visualization: {e}")
                    result_img_path = None

            # Display results
            if result_img_path and os.path.exists(result_img_path):
                st.markdown("### üéØ Detection Results")
                
                with open(result_img_path, 'rb') as f:
                    img_bytes = f.read()
                st.image(img_bytes, caption="Detected Objects with SAHI", use_container_width=True)
                
                # Download button
                st.download_button(
                    label="üì• Download Annotated Image",
                    data=img_bytes,
                    file_name=f"detected_{uploaded_image.name}",
                    mime="image/png"
                )
                
                # Object counts
                st.markdown("### üìä Detection Summary")
                if hasattr(result, 'object_prediction_list') and result.object_prediction_list:
                    class_names = [pred.category.name for pred in result.object_prediction_list]
                    class_counts = Counter(class_names)
                    
                    for cls, count in class_counts.items():
                        st.markdown(f"- **{cls}**: {count}")
                    
                    st.markdown(f"**Total objects detected:** {len(result.object_prediction_list)}")
                else:
                    st.markdown("No objects detected in the image.")
            else:
                st.error("‚ùå Could not display results. Please try again.")
                
        except Exception as e:
            st.error(f"Error processing image: {str(e)}")

# Video Processing Configuration
skip_frames = 2 
max_file_size = st.sidebar.slider(
    "Max video file size (MB)", 
    min_value=10, 
    max_value=500, 
    value=100, 
    step=10
)

def process_video_with_yolo_deepsort(video_path, output_path, conf, selected_model, category_names=None):
    """Process video with YOLO detection and DeepSort tracking"""
    try:
        # Initialize model
        if selected_model == "Default Detection":
            if not os.path.exists(model_path):
                st.error(f"Model file not found: {model_path}")
                return None, None
            model = YOLOWorld(model_path)
        else:  # Text-prompt Detection
            if not category_names:
                st.error("Please enter class names for text-prompt detection")
                return None, None
            if not os.path.exists(text_prompt_model_path):
                st.error(f"Model file not found: {text_prompt_model_path}")
                return None, None
            model = YOLOWorld(text_prompt_model_path)
            model.set_classes(category_names)
        
        # Move model to appropriate device
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        model.model.to(device)
        
        # Initialize tracker
        tracker = DeepSort(max_age=10)
        
        # Open video
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            st.error("Could not open the uploaded video. Please try with a different file.")
            return None, None
        
        # Get video properties
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # Handle invalid fps
        if fps is None or fps <= 0 or np.isnan(fps):
            fps = 24.0
        
        # Setup video writer
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        if not out.isOpened():
            st.error("Could not initialize video writer")
            cap.release()
            return None, None
        
        frame_count = 0
        prev_tracks = []
        detection_counts = Counter()
        
        # Progress bar
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # Update progress
            if total_frames > 0:
                progress = min(frame_count / total_frames, 1.0)
                progress_bar.progress(progress)
                status_text.text(f"Processing frame {frame_count}/{total_frames}")
            
            # Run detection every skip_frames
            if frame_count % skip_frames == 0:
                try:
                    results = model.predict(
                        frame, 
                        conf=conf, 
                        iou=0.6, 
                        augment=False, 
                        verbose=False
                    )
                    
                    if results and len(results) > 0:
                        results = results[0]
                        
                        detections = []
                        if results.boxes is not None and len(results.boxes) > 0:
                            for box in results.boxes:
                                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
                                conf_score = float(box.conf[0].cpu().numpy())
                                cls = int(box.cls[0].cpu().numpy())
                                
                                # Get class name
                                if selected_model == "Default Detection":
                                    class_name = model.names.get(cls, f"class_{cls}")
                                else:
                                    class_name = category_names[cls] if cls < len(category_names) else f"class_{cls}"
                                
                                detection_counts[class_name] += 1
                                detections.append(([x1, y1, x2 - x1, y2 - y1], conf_score, cls))
                        
                        # Update tracker
                        tracks = tracker.update_tracks(detections, frame=frame)
                        prev_tracks = tracks
                    else:
                        tracks = prev_tracks
                        
                except Exception as e:
                    # Use previous tracks if detection fails
                    tracks = prev_tracks
            else:
                tracks = prev_tracks
            
            # Draw tracking results
            for track in tracks:
                if not track.is_confirmed():
                    continue
                    
                ltrb = track.to_ltrb()
                l, t, r, b = map(int, ltrb)
                track_id = track.track_id
                
                # Draw bounding box
                cv2.rectangle(frame, (l, t), (r, b), (0, 255, 0), 2)
                
                # Draw track ID
                label = f"ID: {track_id}"
                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                cv2.rectangle(frame, (l, t - label_size[1] - 10), 
                             (l + label_size[0], t), (0, 255, 0), -1)
                cv2.putText(frame, label, (l, t - 5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
            
            # Write frame
            out.write(frame)
        
        # Cleanup
        cap.release()
        out.release()
        progress_bar.empty()
        status_text.empty()
        
        # Verify output file
        if os.path.exists(output_path) and os.path.getsize(output_path) > 1000:
            return output_path, detection_counts
        else:
            return None, None
        
    except Exception as e:
        st.error(f"Error processing video: {str(e)}")
        return None, None
    finally:
        # Cleanup
        if 'cap' in locals():
            cap.release()
        if 'out' in locals():
            out.release()
        # Force garbage collection
        gc.collect()

# Tab 2: Video Processing
with tab2:
    st.markdown(
        '<p style="font-size:22px; font-family:\'Segoe UI\', sans-serif; font-weight:bold; color:#8cc8e6; margin-top:2px;">üé• Upload a drone video</p>',
        unsafe_allow_html=True
    )
    
    uploaded_video = st.file_uploader(
        "Upload Video", 
        type=['mp4', 'avi', 'mov', 'mkv'], 
        key="vid_upload"
    )
    
    if uploaded_video is not None:
        # Check file size
        file_size = len(uploaded_video.getvalue()) / (1024 * 1024)  # Size in MB
        
        if file_size > max_file_size:
            st.error(f"üö´ File too large! Please upload a video smaller than {max_file_size}MB. Current size: {file_size:.1f}MB")
        else:
            st.markdown("### üñºÔ∏è Uploaded Video Preview")
            st.video(uploaded_video)

            # Validation for text prompt detection
            if selected_model == "Text-prompt Detection" and not category_names:
                st.warning("‚ö†Ô∏è Please enter class names in the sidebar for text prompt detection!")
                st.stop()
            elif selected_model == "Text-prompt Detection" and category_names:
                st.info(f"**Detecting:** {', '.join(category_names)}")

            # Process video button
            if st.button("üöÄ Start Video Processing", key="process_video"):
                # Create temporary files
                with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp_vid:
                    video_bytes = uploaded_video.read()
                    tmp_vid.write(video_bytes)
                    temp_video_path = tmp_vid.name

                with tempfile.NamedTemporaryFile(delete=False, suffix="_out.mp4") as tmp_out:
                    temp_output_path = tmp_out.name
                
                try:
                    st.markdown("### üîÑ Processing Video...")
                    
                    result_path, detection_counts = process_video_with_yolo_deepsort(
                        temp_video_path,
                        output_path=temp_output_path,
                        conf=confidence_value,
                        selected_model=selected_model,
                        category_names=category_names
                    )
                    
                    if result_path and os.path.exists(result_path):
                        st.success("‚úÖ Video processed successfully!")
                        
                        st.markdown("### üéØ Processed Video with Tracking")
                        with open(result_path, 'rb') as vid_file:
                            vid_bytes = vid_file.read()
                        
                        if len(vid_bytes) > 1000:  # Check if file has content
                            st.video(vid_bytes)
                            
                            # Download button
                            st.download_button(
                                label="üì• Download Processed Video",
                                data=vid_bytes,
                                file_name=f"tracked_{uploaded_video.name}",
                                mime="video/mp4"
                            )
                            
                            # Detection summary
                            st.markdown("### üìä Detection Summary")
                            if detection_counts:
                                total_detections = sum(detection_counts.values())
                                st.markdown(f"**Total detections:** {total_detections}")
                                st.markdown("**Detections by class:**")
                                for cls, count in detection_counts.most_common():
                                    st.markdown(f"- **{cls}**: {count}")
                            else:
                                st.markdown("No objects detected in the video.")
                        else:
                            st.error("‚ùå Processed video is empty or corrupted.")
                    else:
                        st.error("‚ùå Video processing failed. Please try again with a different video.")
                        
                except Exception as e:
                    st.error(f"‚ö†Ô∏è Video processing failed: {str(e)}")
                
                finally:
                    # Cleanup temporary files
                    try:
                        if os.path.exists(temp_video_path):
                            os.remove(temp_video_path)
                        if os.path.exists(temp_output_path):
                            os.remove(temp_output_path)
                    except Exception as e:
                        st.warning(f"Warning: Could not cleanup temporary files: {e}")

# Footer
st.markdown("---")
st.markdown(
    '<p style="text-align: center; color: #8cc8e6; font-size: 14px;">üöÄ Drone Detection Dashboard | Built with Streamlit & YOLO</p>',
    unsafe_allow_html=True
)
